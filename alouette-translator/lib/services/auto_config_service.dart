import 'dart:async';
import 'package:flutter/foundation.dart';
import '../models/translation_models.dart';
import 'llm_config_service.dart';

/// è‡ªåŠ¨é…ç½®æœåŠ¡ï¼Œè´Ÿè´£åº”ç”¨å¯åŠ¨æ—¶çš„è‡ªåŠ¨è¿æ¥å’Œé…ç½®
class AutoConfigService {
  static const String _defaultServerUrl = 'http://localhost:11434';
  static const String _defaultModel = 'qwen2.5:latest';
  static const String _fallbackModel = 'qwen2.5:1.5b';
  
  final LLMConfigService _llmConfigService = LLMConfigService();
  
  /// è‡ªåŠ¨é…ç½®LLMè¿æ¥
  Future<LLMConfig?> autoConfigureLLM() async {
    try {
      debugPrint('ğŸ”„ Starting automatic LLM configuration...');
      
      // åˆ›å»ºé»˜è®¤é…ç½®
      final defaultConfig = LLMConfig(
        provider: 'ollama',
        serverUrl: _defaultServerUrl,
        selectedModel: '',
      );
      
      // æµ‹è¯•è¿æ¥åˆ°OllamaæœåŠ¡å™¨
      debugPrint('ğŸ” Testing connection to Ollama...');
      final connectionResult = await _llmConfigService.testConnection(defaultConfig);
      
      if (!connectionResult['success']) {
        debugPrint('âŒ Failed to connect to Ollama: ${connectionResult['message']}');
        return null;
      }
      
      final availableModels = connectionResult['models'] as List<String>?;
      if (availableModels == null || availableModels.isEmpty) {
        debugPrint('âŒ No models available on Ollama server');
        return null;
      }
      
      debugPrint('âœ… Connected to Ollama. Available models: ${availableModels.join(', ')}');
      
      // é€‰æ‹©æ¨¡å‹ï¼šä¼˜å…ˆé€‰æ‹©é»˜è®¤æ¨¡å‹ï¼Œå¦åˆ™é€‰æ‹©å¤‡ç”¨æ¨¡å‹ï¼Œæœ€åé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
      String selectedModel;
      if (availableModels.contains(_defaultModel)) {
        selectedModel = _defaultModel;
        debugPrint('âœ… Using preferred model: $selectedModel');
      } else if (availableModels.contains(_fallbackModel)) {
        selectedModel = _fallbackModel;
        debugPrint('âš ï¸ Preferred model not found, using fallback: $selectedModel');
      } else {
        selectedModel = availableModels.first;
        debugPrint('âš ï¸ Neither preferred nor fallback model found, using: $selectedModel');
      }
      
      final finalConfig = LLMConfig(
        provider: 'ollama',
        serverUrl: _defaultServerUrl,
        selectedModel: selectedModel,
      );
      
      debugPrint('ğŸ‰ Auto-configuration completed successfully with model: $selectedModel');
      return finalConfig;
      
    } catch (error) {
      debugPrint('âŒ Auto-configuration failed: $error');
      return null;
    }
  }
  
  /// è·å–è¿æ¥çŠ¶æ€ä¿¡æ¯
  String getConnectionStatusMessage(LLMConfig? config) {
    if (config == null) {
      return 'Auto-configuration failed. Please configure manually.';
    }
    
    return 'Auto-connected to Ollama with model: ${config.selectedModel}';
  }
  
  /// æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰‹åŠ¨é…ç½®
  bool needsManualConfiguration(LLMConfig? config) {
    return config == null;
  }
}
